"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[4456],{331:n=>{n.exports=JSON.parse('{"tag":{"label":"natural-language","permalink":"/tags/natural-language","allTagsPath":"/tags","count":3,"items":[{"id":"modules/module-4-vision-language-action/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Understanding how LLMs perform cognitive planning by translating natural language commands into ROS 2 action sequences.","permalink":"/modules/module-4-vision-language-action/cognitive-planning"},{"id":"modules/module-4-vision-language-action/llm-robotics-convergence","title":"LLM-Robotics Convergence","description":"Understanding how Large Language Models (LLMs) converge with robotics to enable natural language interaction with humanoid robots.","permalink":"/modules/module-4-vision-language-action/llm-robotics-convergence"},{"id":"modules/module-4-vision-language-action/voice-to-action","title":"Voice-to-Action Using OpenAI Whisper","description":"Understanding how OpenAI Whisper enables voice-to-action capabilities for humanoid robots, including the complete pipeline from audio capture to action generation.","permalink":"/modules/module-4-vision-language-action/voice-to-action"}],"unlisted":false}}')}}]);