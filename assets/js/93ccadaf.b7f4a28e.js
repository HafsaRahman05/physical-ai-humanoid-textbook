"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[891],{2812:n=>{n.exports=JSON.parse('{"tag":{"label":"vla","permalink":"/tags/vla","allTagsPath":"/tags","count":9,"items":[{"id":"modules/module-4-vision-language-action/capstone-project","title":"Capstone Project - The Autonomous Humanoid","description":"Complete VLA pipeline demonstration showing how a simulated humanoid robot receives voice commands, plans paths, navigates obstacles, identifies objects, and manipulates them.","permalink":"/modules/module-4-vision-language-action/capstone-project"},{"id":"modules/module-4-vision-language-action/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Understanding how LLMs perform cognitive planning by translating natural language commands into ROS 2 action sequences.","permalink":"/modules/module-4-vision-language-action/cognitive-planning"},{"id":"modules/module-4-vision-language-action/glossary","title":"Glossary - Key Terminology","description":"Key terminology definitions for Module 4: Vision-Language-Action (VLA), including VLA, voice-to-action, cognitive planning, and related concepts.","permalink":"/modules/module-4-vision-language-action/glossary"},{"id":"modules/module-4-vision-language-action/introduction","title":"Introduction - Vision-Language-Action (VLA) Systems","description":"Introduction to Vision-Language-Action (VLA) systems, learning objectives, prerequisites, and module structure for Module 4.","permalink":"/modules/module-4-vision-language-action/introduction"},{"id":"modules/module-4-vision-language-action/llm-robotics-convergence","title":"LLM-Robotics Convergence","description":"Understanding how Large Language Models (LLMs) converge with robotics to enable natural language interaction with humanoid robots.","permalink":"/modules/module-4-vision-language-action/llm-robotics-convergence"},{"id":"modules/module-4-vision-language-action/module-4-vision-language-action","title":"Module 4 - Vision-Language-Action (VLA)","description":"Introduction to Vision-Language-Action (VLA) systems, covering LLM-robotics convergence, voice-to-action, cognitive planning, and complete VLA pipeline integration.","permalink":"/modules/module-4-vision-language-action/"},{"id":"modules/module-4-vision-language-action/module-integration","title":"Module Integration - Connecting VLA to Previous Modules","description":"Understanding how VLA concepts connect to and build upon concepts from Modules 1, 2, and 3, including ROS 2 integration, simulation support, and perception integration.","permalink":"/modules/module-4-vision-language-action/module-integration"},{"id":"modules/module-4-vision-language-action/safety-validation","title":"Safety & Validation of LLM-Generated Plans","description":"Understanding how LLM-generated action plans are validated and executed safely, including plan verification and constraint checking.","permalink":"/modules/module-4-vision-language-action/safety-validation"},{"id":"modules/module-4-vision-language-action/voice-to-action","title":"Voice-to-Action Using OpenAI Whisper","description":"Understanding how OpenAI Whisper enables voice-to-action capabilities for humanoid robots, including the complete pipeline from audio capture to action generation.","permalink":"/modules/module-4-vision-language-action/voice-to-action"}],"unlisted":false}}')}}]);