"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[2135],{9411:o=>{o.exports=JSON.parse('{"tag":{"label":"voice-to-action","permalink":"/physical-ai-humanoid-textbook/tags/voice-to-action","allTagsPath":"/physical-ai-humanoid-textbook/tags","count":3,"items":[{"id":"modules/module-4-vision-language-action/capstone-project","title":"Capstone Project - The Autonomous Humanoid","description":"Complete VLA pipeline demonstration showing how a simulated humanoid robot receives voice commands, plans paths, navigates obstacles, identifies objects, and manipulates them.","permalink":"/physical-ai-humanoid-textbook/modules/module-4-vision-language-action/capstone-project"},{"id":"modules/module-4-vision-language-action/module-4-vision-language-action","title":"Module 4 - Vision-Language-Action (VLA)","description":"Introduction to Vision-Language-Action (VLA) systems, covering LLM-robotics convergence, voice-to-action, cognitive planning, and complete VLA pipeline integration.","permalink":"/physical-ai-humanoid-textbook/modules/module-4-vision-language-action/"},{"id":"modules/module-4-vision-language-action/voice-to-action","title":"Voice-to-Action Using OpenAI Whisper","description":"Understanding how OpenAI Whisper enables voice-to-action capabilities for humanoid robots, including the complete pipeline from audio capture to action generation.","permalink":"/physical-ai-humanoid-textbook/modules/module-4-vision-language-action/voice-to-action"}],"unlisted":false}}')}}]);